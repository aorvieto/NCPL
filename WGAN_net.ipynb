{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Wasserstein GAN (Neural Net Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "from my_optimizers import GD, Adam, Cata\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "nit = 50000\n",
    "bs = 100\n",
    "nexp = 3\n",
    "real_mu = 0\n",
    "real_sigma = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple generator and critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, 5)\n",
    "        self.fc2 = torch.nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "    \n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.theta1 = nn.Parameter(torch.zeros(1).uniform_(-1,1))\n",
    "        self.theta2 = nn.Parameter(torch.zeros(1).uniform_(-1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.theta1*x+self.theta2*x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(settings):\n",
    "    \n",
    "    grad_x = torch.zeros((nit,nexp), requires_grad=False)\n",
    "    grad_y = torch.zeros((nit,nexp), requires_grad=False)\n",
    "    loss_hist = torch.zeros((nit,nexp), requires_grad=False)\n",
    "\n",
    "    for e in range(nexp):\n",
    "        \n",
    "        ### Init Model\n",
    "        \n",
    "        gen = Generator().to(device)\n",
    "        critic = Critic().to(device) \n",
    "        \n",
    "        \n",
    "        ### Init Optimizers\n",
    "       \n",
    "        ### Init optimizers\n",
    "        if settings[\"optim\"] == 'Adam':\n",
    "            tau=settings[\"tau\"]\n",
    "            beta1_s = settings[\"beta1\"]\n",
    "            beta2_s = settings[\"beta2\"]\n",
    "            opt_gen = Adam(gen.parameters(), lr=tau, betas=(beta1_s, beta2_s), eps = 1e-8)\n",
    "            opt_critic = Adam(critic.parameters(), lr=tau, betas=(beta1_s, beta2_s), eps = 1e-8)\n",
    "            name = r\"\"+settings[\"optim\"]+', $\\\\tau = '+str(settings[\"tau\"])+', \\\\beta_1 = '+str(settings[\"beta1\"])+', \\\\beta_2 = '+str(settings[\"beta2\"])+'$'\n",
    "        \n",
    "        elif settings[\"optim\"] == 'RMSprop':\n",
    "            tau=settings[\"tau\"]\n",
    "            beta1_s = 0\n",
    "            beta2_s = settings[\"beta2\"]\n",
    "            opt_gen = Adam(gen.parameters(), lr=tau, betas=(beta1_s, beta2_s), eps = 1e-8)\n",
    "            opt_critic = Adam(critic.parameters(), lr=tau, betas=(beta1_s, beta2_s), eps = 1e-8)\n",
    "            name = r\"\"+settings[\"optim\"]+', $\\\\tau = '+str(settings[\"tau\"])+', \\\\beta_2 = '+str(settings[\"beta2\"])+'$'\n",
    "\n",
    "        elif settings[\"optim\"] == 'SAGDA':\n",
    "            tau_1=settings[\"tau1\"]\n",
    "            tau_2=settings[\"tau2\"]\n",
    "            opt_gen = GD(gen.parameters(), lr=tau_1)\n",
    "            opt_critic = GD(critic.parameters(), lr=tau_2)\n",
    "            name = r\"\"+settings[\"optim\"]+', $\\\\tau_1 = '+str(settings[\"tau1\"])+', \\\\tau_2 = '+str(settings[\"tau2\"])+'$'\n",
    "\n",
    "        elif settings[\"optim\"] == 'Smooth-SAGDA':\n",
    "            tau_1 = settings[\"tau1\"]\n",
    "            tau_2 = settings[\"tau2\"]\n",
    "            beta_s = settings[\"beta\"]\n",
    "            P_s = settings[\"P\"] \n",
    "            opt_gen = Cata(gen.parameters(), lr=tau_1, beta = beta_s, P = P_s)\n",
    "            opt_critic = GD(critic.parameters(), lr=tau_2)\n",
    "            name = r\"\"+settings[\"optim\"]+', $\\\\tau_1 = '+str(settings[\"tau1\"])+', \\\\tau_2 = '+str(settings[\"tau2\"])+', \\\\beta = '+str(settings[\"beta\"])+', P = '+str(settings[\"P\"])+'$'\n",
    "\n",
    "        else:\n",
    "            print('ERROR, optimizer not defined')\n",
    "                   \n",
    "\n",
    "        ### Optimization\n",
    "        \n",
    "        for i in range(nit):\n",
    "            \n",
    "            z = torch.zeros(bs).normal_(0,1).reshape(-1,1)\n",
    "            real = real_mu+ real_sigma*z\n",
    "            \n",
    "            #critic update\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(gen(z)).reshape(-1)\n",
    "            loss_critic = -torch.mean(critic_real) + torch.mean(critic_fake)+0.001*(critic.theta1**2+critic.theta2**2)\n",
    "\n",
    "            def closure_critic():\n",
    "                gen.zero_grad()\n",
    "                critic.zero_grad()\n",
    "                loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step(closure_critic)\n",
    "            \n",
    "            #generator update\n",
    "            gen_fake = critic(gen(z)).reshape(-1)\n",
    "            loss_gen = -torch.mean(gen_fake)#-0.001*(gen.linear.bias**2+gen.linear.weight**2)\n",
    "            def closure_gen():\n",
    "                gen.zero_grad()\n",
    "                critic.zero_grad()\n",
    "                loss_gen.backward()\n",
    "            opt_gen.step(closure_gen)      \n",
    "            \n",
    "            #saving gradients\n",
    "            with torch.no_grad():\n",
    "                gx = [p.grad.data.detach().numpy() for p in critic.parameters()]\n",
    "                gx = np.concatenate(gx, axis=None)\n",
    "                grad_x[i,e] = torch.tensor(np.linalg.norm(gx))\n",
    "                gy = [p.grad.data.detach().numpy() for p in gen.parameters()]\n",
    "                gy = np.concatenate(gy, axis=None)\n",
    "                grad_y[i,e] = torch.tensor(np.linalg.norm(gy))\n",
    "                est_mu = torch.mean(gen(z))\n",
    "                est_sigma = torch.std(gen(z))\n",
    "                loss_hist[i,e] = torch.abs(est_mu-real_mu)**2+torch.abs(torch.abs(est_sigma)-real_sigma)**2\n",
    "\n",
    "    return [name,loss_hist,grad_x,grad_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orvi/Desktop/smooth-minmax/my_optimizers.py:105: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    }
   ],
   "source": [
    "to_run=[]\n",
    "\n",
    "to_run.append({\"optim\":\"SAGDA\", \"tau1\": 1e-1, \"tau2\": 5e-1})\n",
    "to_run.append({\"optim\":\"RMSprop\", \"tau\": 1e-3, \"beta2\": 0.9 })\n",
    "to_run.append({\"optim\":\"Adam\", \"tau\": 1e-3, \"beta1\": 0.5, \"beta2\": 0.9 })\n",
    "to_run.append({\"optim\":\"Smooth-SAGDA\", \"tau1\": 1e-1, \"tau2\": 1e-1, \"beta\":0.5, \"P\":10})\n",
    "\n",
    "gx = []\n",
    "gy = []\n",
    "names = []\n",
    "loss = []\n",
    "\n",
    "for i in range(len(to_run)):\n",
    "    names_c,loss_c, gx_c, gy_c = train(to_run[i])\n",
    "    names.append(names_c)\n",
    "    loss.append(names_c)\n",
    "    gx.append(gx_c)\n",
    "    gy.append(gy_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.4\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "markers = [\"v\",\"^\",\"<\",\">\",\"o\",\"s\",\"p\",\"P\",\"*\"]\n",
    "colors = sns.color_palette('colorblind')\n",
    "\n",
    "for i in range(len(to_run)):\n",
    "    mean_x_log = np.mean(np.log10(gx[i].detach().numpy()),1)\n",
    "    std_x_log = np.std(np.log10(gx[i].detach().numpy()), 1)\n",
    "\n",
    "    mean_y_log = np.mean(np.log10(gy[i].detach().numpy()),1)\n",
    "    std_y_log = np.std(np.log10(gy[i].detach().numpy()), 1)\n",
    "    \n",
    "    mean_x_log_s = savgol_filter(mean_x_log, 601, 3, mode='nearest')\n",
    "    mean_y_log_s = savgol_filter(mean_y_log, 601, 3, mode='nearest')\n",
    "\n",
    "    std_x_log_s = savgol_filter(std_x_log, 601, 3, mode='nearest')\n",
    "    std_y_log_s = savgol_filter(std_y_log, 601, 3, mode='nearest')  \n",
    "\n",
    "    if i==(len(to_run)-1):\n",
    "        cc='#695025'\n",
    "        ls='dotted'\n",
    "    else:\n",
    "        cc = colors[i%10]\n",
    "        ls='-'\n",
    "\n",
    "    ## Plotting x\n",
    "    ax = plt.subplot(121)\n",
    "    plt.plot(range(nit),np.power(10,mean_x_log_s), linestyle=ls, marker = markers[i%7], label=names[i],linewidth=3, color = cc, markevery=10000, markersize = 12)\n",
    "    plt.fill_between(range(nit),np.power(10,mean_x_log_s-scale*std_x_log_s) , np.power(10,mean_x_log_s+scale*std_x_log_s), alpha=0.5, fc=cc)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(r'$\\Vert\\nabla_x F(x,y)\\Vert$')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "    ## Plotting y\n",
    "    ax = plt.subplot(122)\n",
    "    plt.plot(range(nit),np.power(10,mean_y_log_s), linestyle=ls, marker = markers[i%7], label=names[i],linewidth=3,color =  cc, markevery=10000, markersize = 12)\n",
    "    plt.fill_between(range(nit),np.power(10,mean_y_log_s-scale*std_y_log_s) , np.power(10,mean_y_log_s+scale*std_y_log_s), alpha=0.5, fc= cc)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(r'$\\Vert\\nabla_y F(x,y)\\Vert$')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "    plt.grid()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
