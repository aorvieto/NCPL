{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust non-linear regression on a Gaussian dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from statistics import mean\n",
    "from my_optimizers import GD, Adam, Cata\n",
    "from copy import deepcopy\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "torch.set_deterministic(True)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1.005\n",
    "n = 1000\n",
    "bs = 1000\n",
    "n_batches = math.floor(n/bs)\n",
    "d = 500\n",
    "nit = 2000\n",
    "nexp = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "A = np.random.multivariate_normal(np.zeros(d), np.eye(d), n)\n",
    "x_star = np.random.normal(0, 1, size=d)\n",
    "y_0 = A @ x_star + np.random.normal(0, 100, size=n)\n",
    "y_0 = y_0.reshape(-1,1)\n",
    "A = torch.tensor(A, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden_1=256, n_hidden_2=64, n_output=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = torch.nn.Linear(n_feature, n_hidden_1)\n",
    "        self.hidden_2 = torch.nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.predict = torch.nn.Linear(n_hidden_2, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = self.predict(x)             \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(settings):\n",
    "    \n",
    "    ### Init history variables\n",
    "    grad_x_list = torch.zeros((nit,nexp), requires_grad=False)\n",
    "    grad_y_list = torch.zeros((nit,nexp), requires_grad=False)\n",
    "    \n",
    "    for e in range(nexp):\n",
    "        \n",
    "        ### Init the model\n",
    "        random_state=123\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        net = Net(n_feature=d)\n",
    "        criterion = torch.nn.MSELoss()    \n",
    "        y = y_0 + np.random.normal(0, 10, size=(n,1))\n",
    "        y = torch.tensor(y, requires_grad=False)\n",
    "\n",
    "        ### Init optimizers\n",
    "        if settings[\"optim\"] == 'AGDA':\n",
    "            tau_1=settings[\"tau1\"]\n",
    "            tau_2=settings[\"tau2\"]\n",
    "            optimizer_x = GD(net.parameters(), lr=tau_1)\n",
    "            name = r\"\"+settings[\"optim\"]\n",
    "\n",
    "        elif settings[\"optim\"] == 'Smooth-AGDA':\n",
    "            tau_1 = settings[\"tau1\"]\n",
    "            tau_2 = settings[\"tau2\"]\n",
    "            beta_s = settings[\"beta\"]\n",
    "            P_s = settings[\"P\"]            \n",
    "            optimizer_x = Cata(net.parameters(), lr=tau_1, beta = beta_s, P = P_s)\n",
    "            name = r\"\"+settings[\"optim\"]\n",
    "        else:\n",
    "            print('ERROR, optimizer not defined')\n",
    "\n",
    "        ### Optimization\n",
    "        for i in range(nit):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Sampling batches for this iteration\n",
    "                number_list = range(n)\n",
    "                batch=np.array(random.sample(number_list, bs))\n",
    "\n",
    "                # Data for this batch\n",
    "                X_iter = A[batch,:]\n",
    "                y_iter = y[batch]\n",
    "                y_0_iter = y_0[batch]\n",
    "                \n",
    "            # update for y\n",
    "            with torch.no_grad():\n",
    "                output_ = net(X_iter).detach().numpy()    \n",
    "                y_iter_ = y_iter.detach().numpy()        \n",
    "                grad_y = 2 * (y_iter_ - output_ - lam * (y_iter_ - y_0_iter)) / bs\n",
    "                y_iter_ = y_iter_ + tau_2 * grad_y\n",
    "                y_ = y.detach().numpy() \n",
    "                y_[batch] = y_iter_\n",
    "                y = torch.tensor(y_, requires_grad=False)\n",
    "                \n",
    "            # update for x\n",
    "            net.train()\n",
    "            output = net(X_iter)\n",
    "            loss = criterion(output, y_iter)\n",
    "            def closure_x():\n",
    "                optimizer_x.zero_grad()\n",
    "                loss.backward()\n",
    "            optimizer_x.step(closure_x)\n",
    "\n",
    "            \n",
    "            # compute and store gradient norms\n",
    "            net.zero_grad()\n",
    "            X_eval = copy.deepcopy(A[batch,:])\n",
    "            X_eval.requires_grad = True\n",
    "            y_eval = copy.deepcopy(y[batch])\n",
    "            y_eval.requires_grad = True\n",
    "            y_0_eval = y_0[batch]\n",
    "            output = net(X_eval)\n",
    "            loss = criterion(output, y_eval)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                grad_y = 2 * (y_eval.detach().numpy() - output.detach().numpy() - lam * (y_eval.detach().numpy() - y_0_eval)) / n\n",
    "                grad_x = [p.grad.data.detach().numpy() for p in net.parameters()]\n",
    "                grad_x = np.concatenate(grad_x, axis=None)\n",
    "                grad_x_list[i,e]=np.linalg.norm(grad_x)\n",
    "                grad_y_list[i,e]=np.linalg.norm(grad_y)\n",
    "        \n",
    "    return name, grad_x_list, grad_y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of experiments\n",
    "to_run=[]\n",
    "to_run.append({\"optim\":\"AGDA\", \"tau1\": 0.0005, \"tau2\": 5})\n",
    "to_run.append({\"optim\":\"Smooth-AGDA\", \"tau1\": 0.0005, \"tau2\": 5, \"beta\":0.5, \"P\":20})\n",
    "\n",
    "### Running the experiments\n",
    "gx = []\n",
    "gy = []\n",
    "names = []\n",
    "for i in range(len(to_run)):\n",
    "    names_c, gx_c, gy_c = train(to_run[i])\n",
    "    names.append(names_c)\n",
    "    gx.append(gx_c)\n",
    "    gy.append(gy_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.8\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "\n",
    "markers = [\"v\",\"^\",\"<\",\">\",\"o\",\"s\",\"p\",\"P\",\"*\"]\n",
    "colors = sns.color_palette()\n",
    "\n",
    "for i in range(len(to_run)):\n",
    "    mean_x_log = np.mean(np.log10(gx[i].detach().numpy()),1)\n",
    "    std_x_log = np.std(np.log10(gx[i].detach().numpy()), 1)\n",
    "\n",
    "    mean_y_log = np.mean(np.log10(gy[i].detach().numpy()),1)\n",
    "    std_y_log = np.std(np.log10(gy[i].detach().numpy()), 1)\n",
    "    \n",
    "    mean_x_log_s = savgol_filter(mean_x_log, 301, 3, mode='nearest')\n",
    "    mean_y_log_s = savgol_filter(mean_y_log, 301, 3, mode='nearest')\n",
    "\n",
    "    std_x_log_s = savgol_filter(std_x_log, 301, 3, mode='nearest')\n",
    "    std_y_log_s = savgol_filter(std_y_log, 301, 3, mode='nearest')  \n",
    "    \n",
    "    mean_log_s = mean_x_log_s+mean_y_log_s\n",
    "    std_log_s = std_x_log_s+std_y_log_s\n",
    "\n",
    "    ax = plt.subplot(122)\n",
    "    plt.plot(range(nit),np.power(10,mean_log_s), marker = markers[i%7], label=names[i],linewidth=3,color =  colors[i%10], markevery=500, markersize = 12)\n",
    "    plt.fill_between(range(nit),np.power(10,mean_log_s-scale*std_log_s) , np.power(10,mean_log_s+scale*std_log_s), alpha=0.5, fc= colors[i%10])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(r'$\\Vert\\nabla_x F(x,y)\\Vert+\\Vert\\nabla_y F(x,y)\\Vert$')\n",
    "    plt.title(r'Synthetic Dataset, bs = 1000')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    ax.yaxis.tick_right()\n",
    "    plt.grid()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
